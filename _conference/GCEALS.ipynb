{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee684ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import Latex\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import statistics\n",
    "\n",
    "# set manual seeds for reproducible results\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d697a5a-31b6-470d-bdfc-ef827169a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# GPU / CPU selection\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb0b706-578f-423d-8c67-73821299d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loading function\n",
    "def load_breast_cancer_data():\n",
    "    dbName = 'breast_cancer'\n",
    "    y_column_array = ['target']\n",
    "    data = load_breast_cancer()\n",
    "    df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    df_clean = df.dropna()\n",
    "    y_actual = data.target\n",
    "    \n",
    "    return df_clean.to_numpy(), y_actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cee5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_level,output_level):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_level, output_level),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_level, input_level),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a71594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_level, output_level):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        level_1 = 500\n",
    "        level_2 = 500\n",
    "        level_3 = 2000\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_level, level_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(level_1, level_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(level_2, level_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(level_3, output_level),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_level, level_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(level_3, level_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(level_2, level_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(level_1, input_level),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        \n",
    "        return decoded, encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d8c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp (x) /np.sum (np.exp (x))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1943117-ce54-47b9-a652-0ddffa31c7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "KL Divergence Loss equations for reference\n",
       "\n",
       "\\begin{eqnarray}\n",
       "\\\\\n",
       "p_{ij} =  \\frac{\\exp ({-||x_i - x_j||^2/2 \\sigma^2})}{\\sum_{k\\neq l}\\exp ({-||x_k - x_l||^2/2 \\sigma^2})}\\\\\n",
       "q_{ij} =  \\frac{\\exp ({-||z_i - z_j||^2/2 \\sigma^2}}{\\sum_{k\\neq l}\\exp ({-||z_k - z_l||^2/2 \\sigma^2})}\\\\\n",
       "KL~(P || Q) = \\sum_i \\sum_j p_{ij} log \\frac{p_{ij}}{q_{ij}}\\\\\n",
       "\\\\\n",
       "\\end{eqnarray}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "KL Divergence Loss equations for reference\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\\\\n",
    "p_{ij} =  \\frac{\\exp ({-||x_i - x_j||^2/2 \\sigma^2})}{\\sum_{k\\neq l}\\exp ({-||x_k - x_l||^2/2 \\sigma^2})}\\\\\n",
    "q_{ij} =  \\frac{\\exp ({-||z_i - z_j||^2/2 \\sigma^2}}{\\sum_{k\\neq l}\\exp ({-||z_k - z_l||^2/2 \\sigma^2})}\\\\\n",
    "KL~(P || Q) = \\sum_i \\sum_j p_{ij} log \\frac{p_{ij}}{q_{ij}}\\\\\n",
    "\\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f9e2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(x=None, centroid=None):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
    "    x    : vector or matrix of data with, say, p columns.\n",
    "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - centroid\n",
    "    cov = np.cov(np.transpose(x))\n",
    "    inv_covmat = sp.linalg.pinv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9964360b-6df3-4c37-b7e6-7c7a5e725d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDiv_p(X, n_clusters):\n",
    "    EPS = 1e-12\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # find cluster labels of x using gmm or kmeans\n",
    "    if cluster_method == 'kmeans':\n",
    "        kmeans_x = KMeans(n_clusters = n_clusters, init='k-means++',random_state=123)\n",
    "        labels_x = kmeans_x.fit_predict(X)\n",
    "    else:\n",
    "        # replacing with gmm.cluster_centers_ won't work\n",
    "        gmm_x = GaussianMixture(n_components=n_clusters, random_state=123)\n",
    "        labels_x = gmm_x.fit_predict(X)\n",
    "    \n",
    "    # calculate cluster centers  \n",
    "    x_centroids = []\n",
    "    for j in range(n_clusters):\n",
    "        cluster_samples_x = X[labels_x == j]\n",
    "        cluster_centroid_x = np.mean(cluster_samples_x, axis = 0)\n",
    "        x_centroids.append(cluster_centroid_x)\n",
    "    \n",
    "    x_centroids = np.array(x_centroids)\n",
    "    \n",
    "    \n",
    "    # calculate mahalanobis distance of samples from cluster center\n",
    "    mahal_p = []\n",
    "    for j in range(n_clusters):\n",
    "        mahal_p.append(mahalanobis(X, x_centroids[j]))\n",
    "    \n",
    "    # calculate p numerator and denominator\n",
    "    mahal_p = np.array(mahal_p)\n",
    "    mahal_p = mahal_p * -1\n",
    "    p_numerator = np.exp (np.add(mahal_p,EPS))\n",
    "    p_denominator = np.sum (p_numerator, axis=0)\n",
    "            \n",
    "    # p is the target distribution\n",
    "    p = np.zeros([n_samples, n_clusters])\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_clusters):\n",
    "            p[i][j] = p_numerator[j][i] / p_denominator[i]\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e9f5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDiv_loss(latent, n_clusters, p):\n",
    "    EPS = 1e-12\n",
    "\n",
    "    n_samples = latent.shape[0]\n",
    "    \n",
    "    # find cluster labels of z using gmm or kmeans\n",
    "    if cluster_method == 'kmeans':\n",
    "        kmeans_z = KMeans(n_clusters = n_clusters, init='k-means++',random_state=123)\n",
    "        labels_z = kmeans_z.fit_predict(latent)\n",
    "    else:\n",
    "        gmm_z = GaussianMixture(n_components=n_clusters, random_state=123)\n",
    "        labels_z = gmm_z.fit_predict(latent)\n",
    "        \n",
    "    # calculate cluster centers    \n",
    "    z_centroids = []\n",
    "    for j in range(n_clusters):    \n",
    "        cluster_samples_z = latent[labels_z == j]\n",
    "        cluster_centroid_z = np.mean(cluster_samples_z, axis = 0)\n",
    "        z_centroids.append(cluster_centroid_z)\n",
    "    \n",
    "    z_centroids = np.array(z_centroids)\n",
    "    \n",
    "    # calculate mahalanobis distance of samples from cluster center\n",
    "    mahal_q = []\n",
    "    for j in range(n_clusters):\n",
    "        mahal_q.append(mahalanobis(latent, z_centroids[j]))\n",
    "    \n",
    "    # calculate q numerator and denominator\n",
    "    mahal_q = np.array(mahal_q)\n",
    "    mahal_q = mahal_q * -1\n",
    "    q_numerator = np.exp (np.add(mahal_q,EPS))\n",
    "    q_denominator = np.sum (q_numerator, axis=0)\n",
    "\n",
    "    # q is the predicted distribtuion\n",
    "    q = np.zeros([n_samples, n_clusters])\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_clusters):\n",
    "            q[i][j] = q_numerator[j][i] / q_denominator[i]\n",
    "\n",
    "    # calculate KL div\n",
    "    K_L_div = 0\n",
    "    for i in range(q.shape[0]):\n",
    "        row_sum = 0\n",
    "        for j in range(q.shape[1]):\n",
    "            row_sum += p[i][j] * np.log(p[i][j] /q[i][j])\n",
    "        K_L_div+= row_sum\n",
    "    K_L_div = K_L_div / (n_samples * n_clusters)\n",
    "    \n",
    "    return K_L_div, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b69f73ea-f154-4f5e-af6a-64e321dd80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_testing(dataset, latent_dim, gamma_epoch, l_rate, result_to_csv):\n",
    "    \n",
    "    acc_folds= []\n",
    "    nmi_folds = []\n",
    "    ari_folds = []\n",
    "    fold_counter = 0\n",
    "    \n",
    "    X, y_actual = load_breast_cancer_data()\n",
    "    no_of_clusters = len(np.unique(y_actual))\n",
    "    \n",
    "    kfold = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "        \n",
    "    # gamma calculation loop\n",
    "    for gamma_index, test_index in kfold.split(X):\n",
    "        \n",
    "        # initialize variables at the start of fold\n",
    "        fold_counter +=1\n",
    "        best_gamma = 0\n",
    "        max_acc = 0\n",
    "        \n",
    "        print(\"fold \"+str(fold_counter))\n",
    "        \n",
    "        # separate train and test data\n",
    "        X_test, X_gamma = X[test_index], X[gamma_index]\n",
    "        y_test, y_gamma = y_actual[test_index], y_actual[gamma_index]\n",
    "\n",
    "        # standardize data\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X_gamma = scaler.fit_transform(X_gamma)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        data_tensor = torch.from_numpy(X_gamma).float()\n",
    "        \n",
    "        # search for optimal gamma for each fold\n",
    "        gamma_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        pbar = tqdm(gamma_range)\n",
    "        # --------------------training section--------------------------\n",
    "        for gamma in pbar:\n",
    "            \n",
    "            ae_loss = []\n",
    "            kl_loss = []\n",
    "            ae_kl_loss = []\n",
    "\n",
    "            # initialize model\n",
    "            model = Deep_Autoencoder(input_level = data_tensor.size()[1], output_level = latent_dim)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = l_rate)\n",
    "\n",
    "            # to run code on gpu\n",
    "            model = model.to(device)\n",
    "            data_tensor = data_tensor.to(device)\n",
    "\n",
    "            # pre-calculate p outside the loop once as it doesn't change over epochs\n",
    "            p = KLDiv_p(X = X_gamma, n_clusters = no_of_clusters)\n",
    "            \n",
    "            # train for 5000 epochs\n",
    "            for epoch in range(gamma_epoch):\n",
    "\n",
    "                recon, latent = model(data_tensor) \n",
    "                latent_numpy = latent.cpu().detach().numpy()\n",
    "                #calculate q and kld loss inside the training loop\n",
    "                K_L_div, q = KLDiv_loss(latent = latent_numpy, n_clusters = no_of_clusters, p=p)        \n",
    "\n",
    "                # calculate combined loss using reconstruction loss and KL divergence loss\n",
    "                AE_loss = criterion(recon, data_tensor) \n",
    "                loss = AE_loss + gamma * K_L_div\n",
    "\n",
    "                ae_loss.append(AE_loss.cpu().detach().numpy())\n",
    "                kl_loss.append(gamma * K_L_div)\n",
    "                ae_kl_loss.append(loss.item())\n",
    "\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                y_pred = q.argmax(1)\n",
    "                \n",
    "                # calculate metrics\n",
    "                gamma_acc = np.round(metrics.accuracy_score(y_gamma, y_pred), 4) *100\n",
    "                gamma_nmi = np.round(metrics.normalized_mutual_info_score(y_gamma, y_pred), 4) *100\n",
    "                gamma_ari = np.round(metrics.adjusted_rand_score(y_gamma, y_pred), 4) *100\n",
    "\n",
    "                # save the model with the best accuracy\n",
    "                if gamma_acc > max_acc:\n",
    "                    max_acc =  gamma_acc\n",
    "                    iter_num_max_acc = epoch\n",
    "                    best_gamma = gamma\n",
    "                    torch.save(model.state_dict(), 'Results/' + dataset + '_pretrained_best_model.pt')\n",
    "                    \n",
    "                pbar.set_description(f'gamma={gamma} (best={best_gamma}); acc={gamma_acc:.3f} (best={max_acc:.3f}); epoch={epoch}, loss={loss.item():.3f}')\n",
    "\n",
    "           \n",
    "        # testing\n",
    "        # initialize and load pretrained model\n",
    "        model = Deep_Autoencoder(input_level = data_tensor.size()[1], output_level = latent_dim)\n",
    "        model.load_state_dict(torch.load('Results/' + dataset + '_pretrained_best_model.pt'))\n",
    "\n",
    "        # convert test data to tensor\n",
    "        data_tensor = torch.from_numpy(X_test).float()\n",
    "\n",
    "        # load model and data on device(cpu/gpu)\n",
    "        model = model.to(device)\n",
    "        data_tensor = data_tensor.to(device)\n",
    "\n",
    "        # run model\n",
    "        recon, latent = model(data_tensor)\n",
    "        latent_numpy = latent.cpu().detach().numpy()\n",
    "        \n",
    "        # get test predictions\n",
    "        p = KLDiv_p(X = X_test, n_clusters = no_of_clusters)\n",
    "        K_L_div, q = KLDiv_loss(latent = latent_numpy, n_clusters = no_of_clusters, p = p)\n",
    "        y_pred = q.argmax(1)\n",
    "\n",
    "        # calculate metrics\n",
    "        current_acc = np.round(metrics.accuracy_score(y_test, y_pred), 4) *100\n",
    "        current_nmi = np.round(metrics.normalized_mutual_info_score(y_test, y_pred), 4) *100\n",
    "        current_ari = np.round(metrics.adjusted_rand_score(y_test, y_pred), 4) *100\n",
    "        \n",
    "        acc_folds.append(current_acc)\n",
    "        nmi_folds.append(current_nmi)\n",
    "        ari_folds.append(current_ari)\n",
    "\n",
    "        # Save results in csv\n",
    "        result_to_csv.append(\n",
    "            [\n",
    "                dataset,\n",
    "                latent_dim,\n",
    "                best_gamma,\n",
    "                current_acc,\n",
    "                current_nmi,\n",
    "                current_ari,\n",
    "                iter_num_max_acc\n",
    "\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "    # calculate mean and standard deviation of five folds\n",
    "    result_to_csv.append([\n",
    "        dataset,\n",
    "        latent_dim,\n",
    "        '-',\n",
    "        str(np.round(statistics.mean(acc_folds), 2)) + \" (\" + str(np.round(statistics.pstdev(acc_folds), 2))+\")\",\n",
    "        str(np.round(statistics.mean(nmi_folds), 2)) + \" (\" + str(np.round(statistics.pstdev(nmi_folds), 2))+\")\",\n",
    "        str(np.round(statistics.mean(ari_folds), 2)) + \" (\" + str(np.round(statistics.pstdev(ari_folds), 2))+\")\"\n",
    "    ])\n",
    "    \n",
    "    return result_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e921035",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = ['breast_cancer']\n",
    "learning_rate = 1e-4\n",
    "cluster_method = 'gmm' # 'gmm' or 'kmeans'\n",
    "latent_dim = 10\n",
    "number_of_epoch = 5000\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    print(dataset)\n",
    "    result_to_csv = []\n",
    "    result_to_csv.append(\n",
    "        [\n",
    "            \"Dataset\",\n",
    "            \"dimension\",\n",
    "            \"gamma\",\n",
    "            \"Accuracy\",\n",
    "            \"NMI\",\n",
    "            \"ARI\",\n",
    "            \"iter_num_max_acc\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result_to_csv = training_and_testing(dataset = dataset , latent_dim = latent_dim,\n",
    "                                            gamma_epoch = 5000, l_rate = learning_rate,\n",
    "                                            result_to_csv=result_to_csv)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    pd.DataFrame(result_to_csv).to_csv(\"Results/gceals_\"+ dataset +\".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustervenv",
   "language": "python",
   "name": "clustervenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
